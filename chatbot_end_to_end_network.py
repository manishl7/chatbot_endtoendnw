# -*- coding: utf-8 -*-
"""chatbot_end_to_end_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oAVnaAfjrD3t6n0sYhebXO5mn2PNktD1
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pickle

cd/content/drive/MyDrive/chatbot

with open ('train_qa.txt','rb')as f:
  train_data=pickle.load(f)

with open('test_qa.txt','rb')as f:
  test_data=pickle.load(f)

type(train_data)

type(test_data)

len(train_data)

len(test_data)

#setting up vocab
vocab=set()
all_data=train_data+test_data

for story,question,answer in all_data:
  vocab=vocab.union(set(story))
  vocab=vocab.union(set(question))

vocab.add('yes')
vocab.add('no')

vocab

vocab_size=len(vocab)+1

max_story_len=max(len(data[0])for data in all_data)

max_question_len=max(len(data[1])for data in all_data)

max_story_len

max_question_len

from keras.preprocessing.text import Tokenizer

from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer(filters=[])

tokenizer.fit_on_texts(vocab)

tokenizer.word_index

def vectorize_stories(data,word_index=tokenizer.word_index,max_story_len=max_story_len,max_question_len=max_question_len):
  X=[] #stories
  Xq=[] #question
  Y=[]  #answers
  for story , query,answer in all_data:
    x=[word_index[word.lower()]for word in story]
    xq=[word_index[word.lower()]for word in query]
    y=np.zeros(len(word_index)+1)
    y[word_index[answer]]=1

    X.append(x)
    Xq.append(xq)
    Y.append(y)

  return(pad_sequences(X,maxlen=max_story_len),pad_sequences(Xq,maxlen=max_question_len),np.array(Y))

inputs_train, queries_train , answers_train = vectorize_stories(train_data)

inputs_test, queries_test , answers_test = vectorize_stories(test_data)

inputs_test

sum(answers_test)

#BUILDING A NETWORK
from keras.models import Sequential,Model
from keras.layers.embeddings import Embedding
from keras.layers import LSTM,dot,concatenate,add,Dense,Dropout,Activation
from keras.layers import Input,Permute

inputs_sequence=Input((max_story_len,))
questions=Input((max_question_len,))

#ENCODER M
input_encoder_m=Sequential()
input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))
input_encoder_m.add(Dropout(0.5))

#ENCODER C
input_encoder_c=Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))
input_encoder_c.add(Dropout(0.5))

#QUESTION ENCODER
question_encoder=Sequential()
question_encoder.add(Embedding(input_dim=vocab_size,output_dim=64,input_length=max_question_len))
question_encoder.add(Dropout(0.5))

#Passing the sequences through encoder
input_encoded_m=input_encoder_m(inputs_sequence)
input_encoded_c=input_encoder_c(inputs_sequence)
question_encoded=question_encoder(questions)

#dot product 
match=dot([input_encoded_m,question_encoded],axes=(2,2))
match=Activation('softmax')(match)

#Adding
response=add([match,input_encoded_c])
response = Permute((2, 1))(response)

#concatenate 
answers=concatenate([response,question_encoded])

#Passing through Layers
answers=LSTM(32)(answers)

answers=Dropout(0.5)(answers)
answers=Dense(vocab_size)(answers)

answers=Activation('softmax')(answers)

#Building the final model
model=Model([inputs_sequence,questions],answers)
model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])

model.summary()

history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=200,validation_data=([inputs_test, queries_test], answers_test))

model.save('chatbot_endtoendmemory.h5')
pickle.dump(tokenizer,open('tokenizer_chatbox','wb'))

predictions=model.predict([inputs_test,queries_test])

predictions[0][0]

' '.join(test_data[0][0])

' '.join(test_data[0][1])

' '.join(test_data[0][2])

val_max=np.argmax(predictions[0])

for key,value in tokenizer.word_index.items():
  if value==val_max:
    k=key
print(f"The answer is : {k}" )
print(f"The probabitility is : {predictions[0][val_max]}")

#PREDICTING ON OUR OWN DATA

my_story='Megan is going to the kitchen . Harry is in the bedroom ' #space after the punc so it's same with our training data format
my_question= 'Is Harry in the kitchen ? '

my_story.split()

my_data=[(my_story.split(),my_question.split(),'yes')]

story , query , ans = vectorize_stories(my_data)

pred_results = model.predict(([ story, query]))
val_max = np.argmax(pred_results[0])

for key, val in tokenizer.word_index.items():
    if val == val_max:
        k = key

print("Predicted answer is: ", k)
print("Probability of certainty was: ", pred_results[0][val_max])

#checking the same again
my_story = "John left the kitchen . Sandra dropped the football in the garden ."
my_question = "Is the football in the kitchen ?"
mydata = [(my_story.split(),my_question.split(),'yes')]
story1 , query1 , ans1 = vectorize_stories(my_data1)
my_story,my_ques,my_ans = vectorize_stories(mydata)
#Predicting Result
pred_results = model.predict(([ my_story, my_ques]))
#Generating Result
val_max = np.argmax(pred_results[0])

for key, val in tokenizer.word_index.items():
    if val == val_max:
        k = key

print("Predicted answer is: ", k)
print("Probability of certainty was: ", pred_results[0][val_max])





































































































































dir

cd